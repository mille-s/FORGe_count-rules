{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg2HkOODO/hQOhbhrtrTgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/FORGe_count-rules/blob/main/FORGe_count_rules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a zipped folder with rules and unzip by running this cell\n",
        "! unzip /content/rule.zip"
      ],
      "metadata": {
        "id": "C-RXAe9OceNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import split\n",
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import codecs\n",
        "import re\n",
        "import itertools\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# RE for number of LUs in lexicons: ^\\s*\"[^\"]+_[A-Z]{2}_[0-9]+\"\n",
        "\n",
        "folderPath = '/content/rule'\n",
        "encoding = 'utf-8'\n",
        "debug = 'no'\n",
        "\n",
        "pathOut = '/content/out.txt'\n",
        "\n",
        "# Delete existing property file\n",
        "if os.path.exists(pathOut):\n",
        "  os.remove(pathOut)\n",
        "\n",
        "list_filepaths = sorted(glob.glob(os.path.join(folderPath, '*.*')))\n",
        "#V4Design D5.3\n",
        "language_IDs = ['CA', 'DE', 'EL', 'EN', 'ES', 'GA', 'FR', 'IT', 'PL', 'PT']\n",
        "print(\"WARNING! Check that the list of languages is complete: \"+str(language_IDs))\n",
        "\n",
        "rule_all_count = []\n",
        "rule_con_count = []\n",
        "rule_agg_count = []\n",
        "rule_dsynt_count = []\n",
        "rule_ssynt_count = []\n",
        "rule_lin_count = []\n",
        "rule_sent_count = []\n",
        "\n",
        "def make_stats(list_prefixes_rules, list_prefixes_rules_meta, fo, level):\n",
        "  num_all = int(len(list_prefixes_rules))\n",
        "  num_lang_ind = int(list_prefixes_rules.count('GEN'))\n",
        "  num_lang_spec = int(num_all - num_lang_ind)\n",
        "  if level == 'all':\n",
        "    print('# Rules: ' +str(num_all))\n",
        "    print('# Language-independent rules: ' +str(num_lang_ind))\n",
        "    print('% Language-independent rules: ' +str(round(100*float(num_lang_ind)/float(num_all), 2)) +'%')\n",
        "    print('# Language-specific rules: ' +str(num_lang_spec))\n",
        "    print('% Language-specific rules: '+str(round(100*float(num_lang_spec)/float(num_all), 2)) +'%')\n",
        "    #print('# Meta rules: ' +str(list_prefixes_rules_meta))\n",
        "  fo.write('# Rules: '+str(num_all)+'\\n')\n",
        "  fo.write('# Language-independent rules: '+str(num_lang_ind)+'\\n')\n",
        "  fo.write('% Language-independent rules: '+str(round(100*float(num_lang_ind)/float(num_all), 2))+'%\\n')\n",
        "  fo.write('# Language-specific rules: '+str(num_lang_spec)+'\\n')\n",
        "  fo.write('% Language-specific rules: '+str(round(100*float(num_lang_spec)/float(num_all), 2))+'%\\n')\n",
        "    \n",
        "def fill_lists_rule_count(filename, prefix, list_all, list_con, list_agg, list_dsynt, list_ssynt, list_lin, list_sent):\n",
        "  list_all.append(prefix)\n",
        "  if re.search('Con_Sem', filename) or re.search('Sem[^_]*_Sem', filename):\n",
        "    list_con.append(prefix)\n",
        "  elif re.search('Con_Agg[1-9]', filename):\n",
        "    list_agg.append(prefix)\n",
        "  elif re.search('SemComm_DSynt', filename):\n",
        "    list_dsynt.append(prefix)\n",
        "  elif re.search('DSynt_SSynt', filename) or re.search('SSynt_PostProc', filename) or re.search('SSynt_Agg', filename):\n",
        "    list_ssynt.append(prefix)\n",
        "  elif re.search('SSynt_DMorph_linearize', filename):\n",
        "    list_lin.append(prefix)\n",
        "  elif re.search('S[mM]orph', filename):\n",
        "    list_sent.append(prefix)\n",
        "\n",
        "#print('2 Arguments needed: pathInputFolder encoding')\n",
        "ngrammars = 0\n",
        "fo = codecs.open(pathOut, 'a', 'utf-8')\n",
        "for filepath in list_filepaths:\n",
        "  filename = filepath.rsplit('/', 1)[1]\n",
        "  if debug == 'yes':\n",
        "    print(filename)\n",
        "  if filename.startswith(('1', '2', '3', '4', '5', '6', '7', '8', '9', '0')):\n",
        "    ngrammars = ngrammars + 1\n",
        "    fo.write('--------------\\n')\n",
        "    fo.write(filename+'\\n')\n",
        "    fo.write('--------------\\n')\n",
        "    fd = codecs.open(filepath, 'r', encoding)\n",
        "    rule_grammar_count = []\n",
        "    rule_meta_count = 0\n",
        "    lines = fd.readlines()\n",
        "    # store the language prefix of each rule\n",
        "    # we start counting at 2 because \"leftside\" can only happen on line 3 earliest\n",
        "    x = 2\n",
        "    while x < len(lines):\n",
        "      if re.search('^\\s*leftside\\s*=\\s*\\[\\r*\\n', lines[x]):\n",
        "        if re.search('^[^\\]\\n]', lines[x+1]):\n",
        "          # \\ufeff is BOM\n",
        "          if re.search('^\\ufeff*\\s*[a-zA-Z]+<=>[a-zA-Z]+\\s[^\\n]+\\r*\\n', lines[x-2]):\n",
        "            prefix_languages_rule = []\n",
        "            rulename = ''\n",
        "            if re.search(' : ', lines[x-2]):\n",
        "              rulename = lines[x-2].split(' : ')[0].split(' ', 1)[1]\n",
        "            elif  re.search('\\r\\n', lines[x-2]):\n",
        "              rulename = lines[x-2].split('\\r\\n')[0].split(' ', 1)[1]\n",
        "            elif  re.search('\\n', lines[x-2]):\n",
        "              rulename = lines[x-2].split('\\n')[0].split(' ', 1)[1]\n",
        "            # Find all consecutive language prefixes at the beginning of the rulename\n",
        "            split_rulename = ''\n",
        "            if re.search('_', rulename):\n",
        "              split_rulename = rulename.split('_')\n",
        "            else:\n",
        "              split_rulename = [rulename]\n",
        "            c = 0\n",
        "            while c < 1000:\n",
        "              if c < len(split_rulename):\n",
        "                language_prefix = split_rulename[c]\n",
        "                if language_prefix in language_IDs:\n",
        "                  prefix_languages_rule.append(language_prefix)\n",
        "                  c = c + 1\n",
        "                else:\n",
        "                  c = c + 1000\n",
        "              else:\n",
        "                c = c + 1000\n",
        "            if len(prefix_languages_rule) > 0:\n",
        "              whole_language_prefix = '_'.join(prefix_languages_rule)\n",
        "              if debug == 'yes':\n",
        "                print(whole_language_prefix)\n",
        "              fill_lists_rule_count(filename, whole_language_prefix, rule_all_count, rule_con_count, rule_agg_count, rule_dsynt_count, rule_ssynt_count, rule_lin_count, rule_sent_count)\n",
        "              rule_grammar_count.append(whole_language_prefix)\n",
        "            else:\n",
        "              rule_grammar_count.append('GEN')\n",
        "              fill_lists_rule_count(filename, 'GEN', rule_all_count, rule_con_count, rule_agg_count, rule_dsynt_count, rule_ssynt_count, rule_lin_count, rule_sent_count)\n",
        "            x = x + 1\n",
        "          else:\n",
        "            x = x + 1\n",
        "        else:\n",
        "          rule_meta_count = rule_meta_count + 1\n",
        "          x = x + 1\n",
        "      else:\n",
        "        x = x + 1\n",
        "      \n",
        "    if len(rule_grammar_count) > 0:\n",
        "      make_stats(rule_grammar_count, rule_meta_count, fo, str(filename))\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "dot = '--------------\\n'\n",
        "noRule = 'No rules found!\\n'\n",
        "fo.write(dot)\n",
        "allRules = '\\n\\nAll rulesets ('+str(ngrammars)+' grammars)'\n",
        "print(allRules+'--------------')\n",
        "fo.write(allRules+'\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_all_count) > 0:\n",
        "    make_stats(rule_all_count, '', fo, 'all')\n",
        "else:\n",
        "    print(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('Con rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_con_count) > 0:\n",
        "    make_stats(rule_con_count, '', fo, 'con')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('Agg rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_agg_count) > 0:\n",
        "    make_stats(rule_agg_count, '', fo, 'agg')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('DSynt rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_dsynt_count) > 0:\n",
        "    make_stats(rule_dsynt_count, '', fo, 'dsynt')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('SSynt rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_ssynt_count) > 0:\n",
        "    make_stats(rule_ssynt_count, '', fo, 'ssynt')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('Lin rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_lin_count) > 0:\n",
        "    make_stats(rule_lin_count, '', fo, 'lin')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.write(dot)\n",
        "fo.write('Sent rulesets\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_sent_count) > 0:\n",
        "    make_stats(rule_sent_count, '', fo, 'sent')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF_WWDV-8qdm",
        "outputId": "ccdab031-e7fa-4b47-fc83-4169a7c85fe6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING! Check that the list of languages is complete: ['CA', 'DE', 'EL', 'EN', 'ES', 'GA', 'FR', 'IT', 'PL', 'PT']\n",
            "\n",
            "\n",
            "All rulesets (16 grammars)--------------\n",
            "# Rules: 2497\n",
            "# Language-independent rules: 1870\n",
            "% Language-independent rules: 74.89%\n",
            "# Language-specific rules: 627\n",
            "% Language-specific rules: 25.11%\n"
          ]
        }
      ]
    }
  ]
}