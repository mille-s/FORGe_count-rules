{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMM61htgZ1LF3pVblYR3V4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/FORGe_count-rules/blob/main/FORGe_count_rules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload a zipped folder with rules and unzip by running this cell\n",
        "! unzip /content/rule.zip"
      ],
      "metadata": {
        "id": "C-RXAe9OceNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skU6jCdbafTV"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import codecs\n",
        "import re\n",
        "import itertools\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# RE for number of LUs in lexicons: ^\\s*\"[^\"]+_[A-Z]{2}_[0-9]+\"\n",
        "\n",
        "folderPath = '/content/rule'\n",
        "encoding = 'utf-8'\n",
        "\n",
        "pathOut = '/content/out.txt'\n",
        "\n",
        "# Delete existing property file\n",
        "if os.path.exists(pathOut):\n",
        "  os.remove(pathOut)\n",
        "\n",
        "list_filepaths = sorted(glob.glob(os.path.join(folderPath, '*.*')))\n",
        "#V4Design D5.3\n",
        "language_IDs = ['CA', 'DE', 'EL', 'EN', 'ES', 'FR', 'IT', 'PL']\n",
        "print(\"WARNING! Check that the list of languages is complete: \"+str(language_IDs))\n",
        "#build a dictionary with all language combinations possible\n",
        "language_IDs_combinations = []\n",
        "for L in range(0, len(language_IDs)+1):\n",
        "    for subset in itertools.combinations(language_IDs, L):\n",
        "        language_IDs_combinations.append('_'.join(subset))\n",
        "#language_IDs_combinations = filter(None, language_IDs_combinations)\n",
        "language_IDs_combinations = list(filter(None, language_IDs_combinations))\n",
        "\n",
        "rule_all_count = []\n",
        "rule_con_count = []\n",
        "rule_agg_count = []\n",
        "rule_dsynt_count = []\n",
        "rule_ssynt_count = []\n",
        "rule_lin_count = []\n",
        "rule_sent_count = []\n",
        "\n",
        "def make_stats(list_prefixes_rules, list_prefixes_rules_meta, fo, level):\n",
        "    num_all = int(len(list_prefixes_rules))\n",
        "    num_lang_ind = int(list_prefixes_rules.count('GEN'))\n",
        "    num_lang_spec = int(num_all - num_lang_ind)\n",
        "    if level == 'all':\n",
        "      print('# Rules: ' +str(num_all))\n",
        "      print('# Language-independent rules: ' +str(num_lang_ind))\n",
        "      print('% Language-independent rules: ' +str(round(100*float(num_lang_ind)/float(num_all), 2)) +'%')\n",
        "      print('# Language-specific rules: ' +str(num_lang_spec))\n",
        "      print('% Language-specific rules: '+str(round(100*float(num_lang_spec)/float(num_all), 2)) +'%')\n",
        "      #print('# Meta rules: ' +str(list_prefixes_rules_meta))\n",
        "    fo.write('# Rules: '+str(num_all)+'\\n')\n",
        "    fo.write('# Language-independent rules: '+str(num_lang_ind)+'\\n')\n",
        "    fo.write('% Language-independent rules: '+str(round(100*float(num_lang_ind)/float(num_all), 2))+'%\\n')\n",
        "    fo.write('# Language-specific rules: '+str(num_lang_spec)+'\\n')\n",
        "    fo.write('% Language-specific rules: '+str(round(100*float(num_lang_spec)/float(num_all), 2))+'%\\n')\n",
        "    \n",
        "def fill_lists_rule_count(filename, prefix, list_all, list_con, list_agg, list_dsynt, list_ssynt, list_lin, list_sent):\n",
        "    list_all.append(prefix)\n",
        "    if re.search('Con_Sem', filename) or re.search('Sem[^_]*_Sem', filename):\n",
        "        list_con.append(prefix)\n",
        "    elif re.search('Con_Agg[1-9]', filename):\n",
        "        list_agg.append(prefix)\n",
        "    elif re.search('SemComm_DSynt', filename):\n",
        "        list_dsynt.append(prefix)\n",
        "    elif re.search('DSynt_SSynt', filename) or re.search('SSynt_PostProc', filename) or re.search('SSynt_Agg', filename):\n",
        "        list_ssynt.append(prefix)\n",
        "    elif re.search('SSynt_DMorph_linearize', filename):\n",
        "        list_lin.append(prefix)\n",
        "    elif re.search('S[mM]orph', filename):\n",
        "        list_sent.append(prefix)\n",
        "\n",
        "#print('2 Arguments needed: pathInputFolder encoding')\n",
        "ngrammars = 0\n",
        "fo = codecs.open(pathOut, 'a', 'utf-8')\n",
        "for filepath in list_filepaths:\n",
        "    filename = filepath.rsplit('/', 1)[1]\n",
        "    if filename.startswith(('1', '2', '3', '4', '5', '6', '7', '8', '9', '0')):\n",
        "        ngrammars = ngrammars + 1\n",
        "        #print('--------------')\n",
        "        fo.write('--------------\\n')\n",
        "        #print(filename)\n",
        "        fo.write(filename+'\\n')\n",
        "        #print('--------------')\n",
        "        fo.write('--------------\\n')\n",
        "        fd = codecs.open(filepath, 'r', encoding)\n",
        "        rule_grammar_count = []\n",
        "        rule_meta_count = 0\n",
        "        lines = fd.readlines()\n",
        "        # store the language prefix of each rule\n",
        "        # we start counting at 2 because \"leftside\" can only happen on line 3 earliest\n",
        "        x = 2\n",
        "        while x < len(lines):\n",
        "            if re.search('^\\s*leftside\\s*=\\s*\\[\\r*\\n', lines[x]):\n",
        "                if re.search('^[^\\]\\n]', lines[x+1]):\n",
        "                    # \\ufeff is BOM\n",
        "                    if re.search('^\\ufeff*\\s*[a-zA-Z]+<=>[a-zA-Z]+\\s[^\\n]+\\r*\\n', lines[x-2]):\n",
        "                        #print(lines[x-2])\n",
        "                        list_prefixes = []\n",
        "                        for language_IDs_combination in language_IDs_combinations:\n",
        "                            if re.search (language_IDs_combination+'_', lines[x-2]):\n",
        "                                list_prefixes.append(language_IDs_combination)\n",
        "                        # keep longest prefix only\n",
        "                        if len(list_prefixes) > 0:\n",
        "                            longest_language_combo = ''\n",
        "                            for prefix in list_prefixes:\n",
        "                                if len(prefix) > len(longest_language_combo):\n",
        "                                    longest_language_combo = prefix\n",
        "                            rule_grammar_count.append(longest_language_combo)\n",
        "                            #rule_all_count.append(longest_language_combo)\n",
        "                            fill_lists_rule_count(filename, longest_language_combo, rule_all_count, rule_con_count, rule_agg_count, rule_dsynt_count, rule_ssynt_count, rule_lin_count, rule_sent_count)\n",
        "                        else:\n",
        "                            rule_grammar_count.append('GEN')\n",
        "                            #rule_all_count.append('GEN')\n",
        "                            fill_lists_rule_count(filename, 'GEN', rule_all_count, rule_con_count, rule_agg_count, rule_dsynt_count, rule_ssynt_count, rule_lin_count, rule_sent_count)\n",
        "                        x = x + 1\n",
        "                    else:\n",
        "                        x = x + 1\n",
        "                else:\n",
        "                    rule_meta_count = rule_meta_count + 1\n",
        "                    x = x + 1\n",
        "            else:\n",
        "                x = x + 1\n",
        "        if len(rule_grammar_count) > 0:\n",
        "            make_stats(rule_grammar_count, rule_meta_count, fo, str(filename))\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "dot = '--------------\\n'\n",
        "noRule = 'No rules found!\\n'\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "allRules = '\\n\\nAll rulesets ('+str(ngrammars)+' grammars)'\n",
        "print(allRules+'--------------')\n",
        "fo.write(allRules+'\\n')\n",
        "fo.write(dot)\n",
        "if len(rule_all_count) > 0:\n",
        "    make_stats(rule_all_count, '', fo, 'all')\n",
        "else:\n",
        "    print(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('Con rulesets')\n",
        "fo.write('Con rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_con_count) > 0:\n",
        "    make_stats(rule_con_count, '', fo, 'con')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('Agg rulesets')\n",
        "fo.write('Agg rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_agg_count) > 0:\n",
        "    make_stats(rule_agg_count, '', fo, 'agg')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('DSynt rulesets')\n",
        "fo.write('DSynt rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_dsynt_count) > 0:\n",
        "    make_stats(rule_dsynt_count, '', fo, 'dsynt')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('SSynt rulesets')\n",
        "fo.write('SSynt rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_ssynt_count) > 0:\n",
        "    make_stats(rule_ssynt_count, '', fo, 'ssynt')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('Lin rulesets')\n",
        "fo.write('Lin rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_lin_count) > 0:\n",
        "    make_stats(rule_lin_count, '', fo, 'lin')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "#print('Sent rulesets')\n",
        "fo.write('Sent rulesets\\n')\n",
        "#print(dot)\n",
        "fo.write(dot)\n",
        "if len(rule_sent_count) > 0:\n",
        "    make_stats(rule_sent_count, '', fo, 'sent')\n",
        "else:\n",
        "    print(noRule)\n",
        "    fo.write(noRule)\n",
        "fo.close()\n",
        "\n",
        "    #fo = codecs.open(os.path.join(outputFolder, filename+'.txt'), 'w', 'utf-8')\n",
        "    #fo.close()"
      ]
    }
  ]
}